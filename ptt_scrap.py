# -*- coding: utf-8 -*-
"""

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BFSGzmG6vwILnhg38N-kY4_NvCaMAKaU
"""

import pandas as pd
import requests
from bs4 import BeautifulSoup
import json
import re
import datetime
import time
from time import sleep
from tqdm import tqdm, trange

url = 'https://www.ptt.cc/bbs/Gossiping/index.html' #ptt八卦版
headers = {'User-Agent': 'Googlebot', }
r = requests.get(url, headers=headers, allow_redirects=True)
soup = BeautifulSoup(r.text, 'html.parser')

print(r.status_code) #查看連線狀況
print(r.history) #查看狀況
print(r.url) #查看連線的網址

##抓標題

def catch_url(url):
  headers = {'User-Agent': 'Googlebot', }
  r = requests.get(url, headers=headers, allow_redirects=True)
  soup = BeautifulSoup(r.text, 'html.parser')
  items = soup.find_all("div", class_="title")    #文章標題
  likecounts = soup.find_all("div", class_="nrec")  #聲量
  artics = soup.find_all("div", class_="author")   #作者
  prdates = soup.find_all("div", class_="date")   #日期
  return url, items, likecounts, artics, prdates

r = requests.get(url, headers=headers, allow_redirects=True)
soup = BeautifulSoup(r.text, 'html.parser')
items = soup.find_all("div", class_="title")    #文章標題
likecounts = soup.find_all("div", class_="nrec")  #聲量
artics = soup.find_all("div", class_="author")   #作者
prdates = soup.find_all("div", class_="date")

# 內文處理
def post_catch(post_url):
  post_r = requests.get(post_url, headers=headers, allow_redirects=True)
  post_soup = BeautifulSoup(post_r.text, 'html.parser')
  post_all = post_soup.find(id="main-content") #抓內文
  return post_all

# 內文文字整理
def sort_out_post(x):
  x = x.text.split('\n')
  if '--' in x:
    x_index = x.index('--')
    x = x[1: x_index - 1]
    x = [[content for content in x if content != '']]
    x = ("，".join(str(content2) for content2 in x)).strip('[]').strip("'").replace("', '", '，')
    x = ' '.join(x.split())
    x = x.replace(' ', '，')
  else:
    x = ''
  return x

def post_time_catch(post_url):
  post_r = requests.get(post_url, headers=headers, allow_redirects=True)
  post_soup = BeautifulSoup(post_r.text, 'html.parser')
  post_date = post_soup.find_all("span", class_="article-meta-value")

  for post_date_text in post_date:
    global y_time
    if ':' in post_date_text.text:
      try:
        y_time = (post_date[-1].text)[-20:-1] + (post_date[-1].text)[-1]
        y_time = datetime.datetime.strptime(y_time, '%b %d %H:%M:%S %Y')
        y_time = y_time.strftime('%Y/%m/%d %H:%M:%S')
      except:
        y_time = ''
    else:
      y_time = ''
  return y_time

# 最新頁定位
url = 'https://www.ptt.cc/bbs/Gossiping/index.html'
ptt_scrap_list = []
page = 0
time_to_re = 0

with tqdm(total=100) as pbar:
  for page in range(1,1500):
    if time_to_re > 20:
      time.sleep(20)
      time_to_re -= 20
    else:
      headers = {'User-Agent': 'Googlebot', }
      r = requests.get(url, headers=headers, allow_redirects=True)
      soup = BeautifulSoup(r.text, 'html.parser')
      items = soup.find_all("div", class_="title")    #文章標題
      likecounts = soup.find_all("div", class_="nrec")  #聲量
      artics = soup.find_all("div", class_="author")   #作者
      prdates = soup.find_all("div", class_="date")

      ## 抓文章
      for item, likecount, artic, prdate in zip(items, likecounts, artics, prdates):
          Title = item.text.replace('\n', '').replace('\r', '')
          Post_type = re.findall((re.compile(r'\[(.*)\]')), Title)
          if bool(Post_type) is True:
            Post_type = Post_type[0]
            Like_count = likecount.text
            Author = artic.text
            if item.select_one("a"):
              Link = 'https://www.ptt.cc' + item.select_one("a").get("href")         
              # 內文區段
              post_all = post_catch(Link)
              Post_Content = sort_out_post(post_all)
              # 內文時間截取
              Post_Time = post_time_catch(Link)
              print(Link)
              
              # 彙整成list
              ptt_scrap_list.append(list([Post_type, Title, Like_count, Author, Post_Time, Link, Post_Content]))

      # 往前一頁
      btn = soup.select('div.btn-group > a')
      up_page_href = btn[3]['href']
      next_page_url = 'https://www.ptt.cc' + up_page_href
      url = next_page_url
      time_to_re += 1
      print(url)
      print('')
      sleep(0.01)
      pbar.update(10/150)

print(bool(Post_type))

ptt_scrap_df = pd.DataFrame(ptt_scrap_list, columns=['Post_Type', 'Title', 'Like_Count', 'Author', 'Post_Time', 'hyperlink', 'Post_Content'])
ptt_scrap_df['Title'] = ptt_scrap_df['Title'].str.replace(r'.*]', '')

ptt_scrap_df

ww = ptt_scrap_df['Title'][55]
ww

#過濾標點符號
import re
r1 = u'[a-zA-Z0-9』!"#$%&\'()*+,-./:;<=>?@，。?★、…【】《》？「」『』！[\\]^_`{|}~]+'
r2 = "[\s+\.\!\/_,$%^*(+\"\']+|[+——！，。？、~@#￥%……&*（）]+"
r3 = "[.!//_,$&%^*()<>+\"'?@#-|:~{}]+|[——！\\\\，。=？、：※「」『』《》【】￥……（）]+"
r4 = "\\【.*?】+|\\《.*?》+|\\#.*?#+|[.!/_,$&%^*()<>+""'?@|:~{}#]+|[——！\\\，。=？、：「」『』￥……（）《》【】]"
cleanr = re.compile('<.*?>')
ww = re.sub(cleanr, ' ', ww)

#去除html標籤
ww = re.sub(r1, '', ww)
ww = re.sub(r2,'', ww)
ww = re.sub(r3,'', ww)
ww = re.sub(r4,'', ww)

print(ww)

#分詞
import jieba
ww_seg = jieba.lcut(ww)
result = ' '.join(ww_seg)
print(result)

# 安裝 huggingface transformer
!pip install transformers

from transformers import pipeline

nlp = pipeline("text-generation")
results = nlp(result, max_length=50, do_sample=False)

print(results[0]['generated_text'])



''''
# 關鍵字搜尋
ptt_scrap_df[ptt_scrap_df['Title'].str.contains('問卦')].head()
''''

payload={'from':'/bbs/Gossiping/index.html',
    'yes':'yes'}

rs = requests.session()
url = rs.post('https://www.ptt.cc/ask/over18',verify = False, data=payload)

r = requests.Session()
payload ={"from":"/bbs/Gossiping/index.html",
    "yes":"yes"}

r1 = r.post("https://www.ptt.cc/ask/over18?from=%2Fbbs%2FGossiping%2Findex.html", payload)
r2 = r.get("https://www.ptt.cc/bbs/Gossiping/M.1605496375.A.DB3.html")
print(r2.text)

url = 'https://www.ptt.cc/bbs/Gossiping/M.1605496375.A.DB3.html'

post_r = requests.get(url, headers=headers, allow_redirects=True)
post_soup = BeautifulSoup(post_r.text, 'html.parser')

postpost = soup.find(class_="f6").find_next('span').contents[0]
postpost

x = post_all[0].text.split('\n')
x_index = x.index('--')
x = x[1: x_index - 1]
x = [[content for content in x if content != '']]
x = ("，".join(str(content2) for content2 in x)).strip('[]').strip("'").replace("', '", '，')
x = ' '.join(x.split())
x = x.replace(' ', '，')

ptt_scrap_df[ptt_scrap_df['Title'].str.contains('刪除')]

#檔案儲存
from google.colab import files

ptt_output = ptt_scrap_df['Post_Content']
ptt_scrap_df['Post_Content'].to_csv('ptt_df.csv')
files.download('ptt_df.csv')

''''
# 文章時間處理
import datetime

y = 'https://www.ptt.cc/bbs/Gossiping/M.1605421882.A.F28.html'
y_r = requests.get(y, headers=headers, allow_redirects=True)
y_soup = BeautifulSoup(y_r.text, 'html.parser')

post_date = y_soup.find_all("span", class_="article-meta-value")
y_time = (post_date[-1].text)[-20:-1] + (post_date[-1].text)[-1]
y_time = datetime.datetime.strptime(y_time, '%b %d %H:%M:%S %Y')
y_time = y_time.strftime('%Y/%m/%d %H:%M:%S')
y_time
''''